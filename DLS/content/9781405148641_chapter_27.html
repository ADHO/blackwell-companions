<div class="content">
                  <h2 class="normal">27. 
                     <h2 class="normal">Writing Machines</h2>
                  </h2>
                  <span style="font-weight:bold;">William Winder</span>
                  
                  <a name="_h63"></a><a name="_h64"></a>
                  
                  
                  
                  
                  <blockquote>
                     <p>They used to say, my friend, that the words of the oak in the holy place of Zeus at Dodona were the first prophetic utterances.
                        The people of that time, not being so wise as you young folks, were content in their simplicity to hear an oak or a rock,
                        provided only it spoke the truth.
                     </p>(<i>Phaedrus</i> 275b—c)
                  </blockquote>
                  
                  
                  <a name="ss1-6-8_h0"></a><h2 class="normal">Writing</h2>
                  
                  <a name="ss1-6-8_h1"></a><h3 class="normal">Writing in a new key</h3>
                  
                  <p>It has been the fleets of humming word processors, not the university mainframe computers nor the roaring newspaper printing
                     presses of the turn of the nineteenth century, that have drawn humanists to writing machines and their industrialized texts
                     (<a href="#ss1-6-8_b41">Winder 2002</a>). We write perhaps less and less, but we process more and more, with computers working quietly in the background. And only
                     in the dead silence of a computer crash where hours of work have disappeared do we understand clearly how much our writing
                     depends on machines. Formatters, spell checkers, thesauri, grammar checkers, and personal printers support our writing almost
                     silently. Yet we suspect that today's ubiquitous editing and display functions will seem quaint in ten years' time, perhaps
                     as quaint and mysterious as the thump of a typewriter's carriage shift.
                  </p>
                  
                  <p>Computers are necessarily writing machines. When computers process words, they generate text and a great deal of it. Library
                     catalogues over the globe spew out countless replies to queries (author, keyword, call number, title, subject heading, year,
                     language, editor, series, …); banking machines unabashedly greet us, enquire discreetly about our password in hushed tones, and remind us not to leave
                     our banking card in the machine when we leave (would you like a receipt?). From the internet come waves of pages crafted for each visitor's communicative needs. Computers, however much they calculate,
                     write even more relentlessly.
                  </p>
                  
                  <p>But are computers typists or writers? We can understand the constant growth of writing, by or through computers, as the effect of two complementary cultural compulsions,
                     each tunneling toward the other from opposing positions and destined to meet ultimately somewhere in an unknown middle ground.
                     The first driving force is the grassroots improvement of computer-assisted writing; the second is artificial intelligence.
                  </p>
                  
                  <p>From the "<b>accelerated writing</b>" paradigm will come the next generation of word processors — true text processors — that will give us advanced tools for automatic generation of text. The computer is much too powerful to remain a simple word
                     processor, offering only word changes such as spelling and thesaurus functions; it can do much more, such as understanding
                     text well enough to propose radical changes in composition.
                  </p>
                  
                  <p>Tools for boilerplate construction are good candidates for the text processor of the near future. If I write a reference letter
                     for one student, Sarah, and then have another student, Ralph, who followed much the same program of study and requires a letter
                     as well, I might cut and paste relevant parts of Sarah's letter into Ralph's. At present I would have the tedious task of
                     making sure all occurrences of "Sarah" are replaced by "Ralph" and that no "she" referring to Sarah is left in the pasted text. The text processor of the future should check the coherence of my discourse
                     and transform the pasted text for me automatically, just as even now grammar checkers automatically suggest changes for agreement
                     between verbs and their subjects. In many languages, such text transformations would not be trivial at all: in French it would
                     require changing all the numerous gender agreements ("son professeur de mathématiques l'a décrite comme une bonne étudiante"); in English, though simpler, it might mean transforming gender-specific adjectives or nouns ("pretty"/"handsome," "waiter"/ "waitress") and possessive adjectives ("him," "his"). At present, crude search and replace functions lead to embarrassing slips: in one republished thesis all the occurrences
                     of "thesis" had been replaced with "book," even in the case of "hypothesis," which became the curious neologism "hypobook" throughout the published text.
                  </p>
                  
                  <p>Creating texts in this accelerated writing paradigm is more about writing text-generating code and using functions that recycle
                     texts into boilerplate components than it is about writing any of the final versions themselves. Separation of content and
                     form is a standard approach for some editing software such as <i>Tex</i> and bibliographic software that generates citations and bibliography. The new text processor will extend this approach by
                     becoming a spreadsheet equivalent for writing: instead of cutting and pasting blocks of text, we will cut and paste text formulae
                     that generate programmatically the final text (see the discussion of a "textual <i>Excel</i>" in Winder forthcoming). We find the infrastructure for this coded text in present-day applications such as automatic translation
                     (increasingly online, such as <i>Babel Fish</i>), formalisms developed in text encoding projects (Unicode standard, <i>Text Encoding Initiative</i>), and programming languages that are increasingly closer to human language (Perl, Ruby, Python; see <a href="#ss1-6-8_b23">Malsky 2006</a>, chapter 3, "Language and I MEAN Language"<a href="#ss1-6-8_fn1"><sup>1</sup></a>).
                  </p>
                  
                  <p>The other, complementary orientation does not have as its goal an incremental enhancement of writers' tools; rather, the goal
                     of the artificial intelligence approach is to construct directly the components of a writing intelligence — machines that have personality and thoughts like humans, not unlike the robots we know from science fiction.
                  </p>
                  
                  <p>These two projects, accelerated writing and artificial intelligence, seem destined to meet. Writers will become increasingly
                     wedded to programming, to code-generating programmer tools (such as databases and version control), and to the industrial
                     production of text, on the one hand, and computers will become increasingly human on the other.
                  </p>
                  
                  <p>But there is a third paradigm, a third perspective, less overtly commercial, but perhaps just as focused on that eventual
                     meeting of minds and machines as the first two: <b>automatic generation of art</b>. What is most human is not just the body's artifacts (enhanced by the versatile printing of the accelerated writing paradigm)
                     nor the calculating mind (studied and re-created in artificial intelligence), but as well that elusive creative spirit that
                     traverses all our endeavors. These three orientations are destined to meet because human value ultimately has its source in
                     precisely what is peculiarly human: a certain confluence of mind, body, and spirit.
                  </p>
                  
                  <p>Our focus here is on automatically produced written art. This is clearly a narrow view of how people communicate in the electronic
                     medium (by definition a multi-media environment) and a radical reduction of the field of generated art. Written language has,
                     however, a singular place in computational systems, if only because writing and reading program code is a faculty shared by
                     both human programmers and computers. As well, generation is perhaps best defined as a reading out of instructions. Printing,
                     as the ultimate and most general reading and writing out of instructions, is where writing, thinking, and creating converge
                     (<a href="#ss1-6-8_b42">Winder 2004</a>: 449).
                  </p>
                  
                  
                  <a name="ss1-6-8_h2"></a><h3 class="normal">Printing, writing, and art</h3>
                  
                  <p>Printing and writing are clearly related. Like printing, writing is machine-mediated language, though the machine may be as
                     simple as a pencil. (Is there writing without printing?) And though we may understand writing as something more than printing, it is often not clear what precisely distinguishes
                     the two. Such is the point that Plato brings home in Socrates's retelling of the myth of the birth of writing. Theuth invents
                     writing as a remedy for memory loss, as if the mind could extend to the page. Thamus, the king of the gods, claims that Theuth's
                     invention is not a tonic for memory but rather a poison, the death of the mind (see the <i>Perseus</i> library (<a href="#ss1-6-8_b9">Crane 2006</a>) for <i>Phaedrus</i>; discussion in <a href="#ss1-6-8_b12">Derrida 1992</a>). Socrates debates the question of the soul of discourse that writing is supposed to capture, that precise place where discourse
                     takes on its own life. He finds there something we will call art.
                  </p>
                  
                  <p>The word "art" has many meanings, which reflect that same liveliness of meaning we find in artistic objects themselves:</p>
                  
                  
                  <p> <b>•</b> <i>Mercantile art</i>: The "pricelessness" of art reflects the fact that it is not produced in the same way as other commercial products (the value of the means of
                     production is not commensurate with the value of the artistic object) and thus art seems to belong to an entirely different
                     economy (the slogan "art for art's sake" is perhaps a reflection of that otherness of art). It is also other in that its uniqueness is seemingly borrowed from the
                     uniqueness and pricelessness of the very life of the artist. Within the commercial system, art is arbitrary because ultimately
                     everything is tied to a unique human existence; anything can be bought and sold as art (witness the "readymade" art of Marcel Duchamp).
                  </p>
                  
                  <p> <b>•</b> <i>Inspirational art</i>: The "liveliness" of art lies in an overabundance made possible by a surprising efficiency, an efficiency (analogous to Freud's economy principle
                     for wit) that does not make sense with respect to the mercantile economy. For example, <a href="#ss1-6-8_b29">Francis Ponge 1999</a>: (346) notes that the French word "oiseau" contains all the vowels (such poetic "trouvailles" generally do not translate; the word in Spanish with all the vowels, written and pronounced, is "murciélago", bat). This is a surprising efficiency in the sense that the otherwise arbitrary word for bird seems to reflect the vocal
                     dimension of its object: a bird is in fact a "vowel" creature. The artistic reflex we cultivate and from which derive a feeling of joy, freedom, generosity, and community, is
                     to recognize and cultivate this "graceful" dimension of the universe, a grace that seems to conspire to harmonize the world in an abundant, costless economy. The artistic
                     vision, as a lay version of the religious experience, adds a layer of meaning that harmonizes the way things are. In the artistic
                     world, nothing is arbitrary, nothing is without meaning, and everything is given freely — if not, it would not be art, it would not be grace. Inspirational art is therefore not arbitrary, but rather <b>motivated</b> by the surprising generosity of the world.
                  </p>
                  
                  <p> <b>•</b> <i>Living art</i>: Art is not only lively, it is "alive" in that, like a living organism, it assimilates other forms of art, adapts itself to new environments, and reproduces itself
                     in derivative works. Unlike living things, however, art has no "fixed address." It does not need any particular material envelope and by nature tends to bleed into all things at all levels of organization,
                     including the spectator. An artistic organization such as Shakespearean style can find itself just as easily in a play as
                     in a novel, in a style of dress, or in a person. Art is universal <b>form</b>.
                  </p>
                  
                  
                  <p>As a universal principle of combination, art's most general aesthetic effect is a feeling of vivifying inclusion — connectedness — that attaches the spectator to the grace (costless harmony) of the world. The artistic text must be coherent and alive at
                     the same time. It is an adaptive structure that is stubbornly the same in spite of the constant changing or shimmering of
                     its being. Art is both process and product: through art we capture open-ended productivity (generation) in a closed system
                     (product).
                  </p>
                  
                  <p>The question we must deal with here is the relation between artful generation and machines. Just as Socrates concluded that
                     writing is discourse without a soul, so we might suspect that machines are writers without creativity. What is artful generation? How do machines simulate, at least, generation? Is there a special kind of artistic generation? A special use of technique?
                  </p>
                  
                  
                  <a name="ss1-6-8_h3"></a><h3 class="normal">Transmutations: things that go generate in the night</h3>
                  
                  <p>Criticizing a poorly organized discourse, Socrates points out that the lines of Midas's epitaph, delivered by the bronze statue
                     of a maiden and reported in Plato's text, can be read in any order:
                  </p>
                  
                  <p>
                     </p>
<blockquote>
                        
                        <p class="stanza">Bronze maiden am I and on Midas's mound I lie.<br>
                           As long as water flows and tall trees bloom,<br>
                           Right here fixed fast on the tearful tomb,<br>
                           I shall announce to all who pass near: Midas is dead and buried here.<br>
                           (Phaedrus 264d; cited and translated in <a href="#ss1-6-8_b8">Carson 1986</a>: 134)<br>
                           
                        </p>
                        
                     </blockquote>
                  
                  
                  <p>How many permutations are there and do they all truly make sense? (There are <i>n</i>! permutations of <i>n</i> things. Here there are 4 sentences and therefore 4! permutations: 4*3*2*1 = 24 altogether.) Does Socrates know how many there really are? Did he actually generate all permutations (etched in stone? on "papyrus"? in his head?) or can he simply "tell" from each sentence? Are some better than others? How are we to judge?
                  </p>
                  
                  <p>We can generate all of the permutations easily enough (see <a href="#ss1-6-8_b43">Winder 2006</a>, <i>Robotic Poetics</i>, for an online generator), but are there not many more "versions" we might want to consider? For instance, the original Greek is certainly a version:
                  </p>
                  
                  <blockquote>
                     
                     <p class="stanza">
                        <br><a name="ss1-6-8_fu1"></a>
                        <img src="9781405148641_chapter_27_fu1.gif">
                        <br><br>
                                     ("Phaedrus" 264d<a href="#ss1-6-8_fn2"><sup>2</sup></a>)<br>
                        
                     </p>
                     
                  </blockquote>
                  
                  <p>And the transliterated Greek:</p>
                  
                  <blockquote>
                     
                     <p class="stanza">
                        chalkê parthenos eimi, Mida d' epi sêmati keimai.<br>
                        ophr' an hudo êr te naêi kai dendrea makra tethêlêi,<br>
                        autou têide menousa poluklautou epi tumbou,<br>
                        angeleo ê pariousi Midas hoti têide tethaptai.<br>
                        ("Phaedrus" 264d<a href="#ss1-6-8_fn3"><sup>3</sup></a>)<br>
                        
                     </p>
                     
                  </blockquote>
                  
                  <p>And perhaps we should even consider the Greek transliteration of the English translation of the Greek!:</p>
                  
                  <p><br><a name="ss1-6-8_fu2"></a>
                     <img src="9781405148641_chapter_27_fu2.gif">
                     <br></p>
                  
                  <p>The latter is generated simply by a switch of fonts in a word processor (see Balpe "Imagination" 16–18). Word processors have a "Midas touch" we are all familiar with (whether they produce gold or not is another question). Text is malleable; one text can be transformed
                     into another; the same thing can be said in different ways. Here is the same English epitaph in the BookDings font:
                  </p>
                  
                  <p><br><a name="ss1-6-8_fu3"></a>
                     <img src="9781405148641_chapter_27_fu3.gif">
                     <br></p>
                  
                  <p><i>JanusNode</i> (Westbury) has an "ee cummings" mode, along with a sophisticated generation scripting language (see below), which will lay out a text in an ee cummings fashion:
                  </p>
                  
                  <p>
                     </p>
<blockquote>
                        
                        <p class="stanza">
                           Bronze maiden<br>
                           am<br>
                           I<br>
                           and<br>
                           on Midas' mound<br>
                           I lie.<br>
                           As<br>
                           long as water flows<br>
                           and tall<br>
                           trees bloom,<br>
                           Right<br>
                           here<br>
                           fixed fast on<br>
                           the tearful<br>
                           tomb,<br>
                           I<br>
                           sh<br>
                           all announce<br>
                           to<br>
                           all<br>
                           who<br>
                           pass<br>
                           near:<br>
                           Midas<br>
                           is<br>
                           dead<br>
                           and<br>
                           buried<br>
                           here.<br>
                           ( <i>JanusNode</i> "ee cummings" style)<br>
                           
                        </p>
                        
                     </blockquote>
                  
                  
                  <p>Such transmutations are perhaps not of themselves art, but they are certainly starting points of creativity. Whether automatic
                     or human, transmutation is most often simply the source of humor and obfuscation. Luis d'Antin van Rooten's <i>Mots d'heures: Gousses, Rames</i> are phonetic transliterations of English "Mother Goose rhymes" into French, which he then presents, with "serious" exegesis, as a critical edition of a collection of medieval French verse. Here is "Humpty Dumpty":
                  </p>
                  
                  <p>
                     </p>
<blockquote>
                        
                        <p class="stanza">
                           Un petit d'un petit<br>
                           S'étonne aux Halles<br>
                           Un petit d'un petit<br>
                           Ah! degrés te fallent [3]<br>
                           Indolent qui ne sort cesse<br>
                           Indolent qui ne se mène<br>
                           Qu'importe un petit d'un petit<br>
                           Tout Gai de Reguennes.<br>
                           ("Poem 1")<br>
                           
                        </p>
                        
                     </blockquote>
                  
                  
                  <p>Exegesis is given in copious footnotes to this "medieval" poem; here is the third annotation [3], an interpretation of line 4:</p>
                  
                  <p>Since this personage [the "petit," offspring of the child marriage ("d'un petit") mentioned in the first line] bears no titles, we are led to believe that the poet writes of one of those unfortunate idiot-children
                     that in olden days existed as a living skeleton in their family's closet. I am inclined to believe, however, that this is
                     a fine piece of misdirection and that the poet is actually writing of some famous political prisoner, or the illegitimate
                     offspring of some noble house. The Man in the Iron Mask, perhaps? ("Poem 1", note 3; pages unnumbered)
                  </p>
                  
                  <p>Russell Horban makes use of the prolific number of graphemes in English (1,120 graphemes for 40 sounds; compared with Italian,
                     which has 33 graphemes for 25 sounds) to transliterate present-day English into a futuristic newspel: "What ben makes tracks for what wil be. Words in the air pirnt foot steps on the groun for us to put our feet in to" (121). In Dunn's <i>Ella Minnow Pea</i> letters of the alphabet are progressively banned from all writings and the eponymous protagonist is reduced to expressing
                     herself in baby talk — "No mo Nollop poo poo!" ["No more Nollop mess"] — and signing her name as "LMNOP" (2001: 197).
                  </p>
                  
                  <p>Transliteration in a more practical vein is found in spelling reform projects such as <i>Truespel</i>. The online <i>Truespel Converter</i> transforms our epitaph into:
                  </p>
                  
                  <p>
                     </p>
<blockquote>
                        
                        <p class="stanza">
                           Braanz maedin am Ie and aan Miedis's mound Ie lie.<br>
                           Az laung az wauter floez and taul treez bluem,<br>
                           Riet heer fiksd fast aan thu teerfool tuem,<br>
                           Ie shal unnounts tue aul hue pas neer: Miedis iz ded and baireed heer.<br>
                           (<a href="#ss1-6-8_b11">Davidson 2006</a>)<br>
                           
                        </p>
                        
                     </blockquote>
                  
                  
                  <p>Transmuted versions proliferate in the machine (the Unicode specification for character sets has over 2<sup>32</sup> possible characters). Word processors can transmute text so effectively because each character is represented by a specific
                     number or "code point" that can be mapped to any other code point. All word processor text is represented in fundamentally the same way: a set of
                     points that can be dressed up in different font garb. Underlying the word processor's transmutations and Socrates's permutations
                     is a very abstract machine, the fundamental machine of transmutation, a mapping between code points: a permutation is a mapping
                     of one line onto another (for instance, mapping line 1 to 2 and line 2 to 1 gives one combination); a symbol transmutation
                     is a mapping of one alphabet onto another.
                  </p>
                  
                  
                  
                  <a name="ss1-6-8_h4"></a><h2 class="normal">Topographies for Transmutation</h2>
                  
                  <p>For mapping to be coherent, automatic techniques depend on the formally described topographies of the source and target texts.
                     Beyond characters, more important linguistic topographies are syntax, lexical meaning, and narrative. Each of these dimensions
                     brings with it its own processing difficulties. We will look at each in turn.
                  </p>
                  
                  <p>One fundamental method of generation is through <b>syntactic templates</b>, essentially the boilerplate texts of word processors. Any number of variant epitaphs can be generated by substituting equivalent
                     terms at appropriate positions in the text (<a href="#ss1-6-8_t1">Table 27.1</a>).
                  </p>
                  
                  <p>This "combinatory of combinations" codes for 4<sup>6</sup> sentences, for a total of 4,096. Here all the combinations are acceptable, but generally syntactic and perhaps semantic constraints
                     will be limiting factors (see <a href="#ss1-6-8_b42">Winder 2004</a>: 458ff and the <i>Robotic Poetics</i> site). As syntactic constraints are reduced, generation becomes increasingly simple. Divinatory systems, such as <i>I Ching</i>, are typically built on units that combine freely, with few or no syntactic constraints: a single narrative unit is attached
                     to each free combination of rods. There are no natural languages that are combinatorially complete in this way, i.e., not
                     all combinations of words make grammatical utterances. Even declined languages like Latin, which have very flexible word order,
                     require a particular mix of nouns and verbs. Perhaps only artificial languages such as <i>I Ching</i> can achieve the ideal of a combinatorially complete language.
                  </p>
                  
                  <p>The systematic choices that are driven by referential and stylistic constraints are part of language use, not of the linguistic
                     system. Syntactic templates are useful starting points for generation because they largely resolve the major systemic linguistic
                     constraints and so set the stage for a free stylistic and referential combinatory. Using such templates <a href="#ss1-6-8_b32">Roubaud 1985</a>: (191, cited in <a href="#ss1-6-8_b7">Braffort 2006</a>) imagined chimeric poetry which combines the style of two poets. One example is the Rimbaude-laire, a set of poems built
                     from templates extracted from Rimbeau's poems and lexical items from Baudelaire's (or vice versa). Roubaud and Lusson developed
                     combinatory software that automatically generates all the permutations of the lexical items within a template (<a href="#ss1-6-8_b33">Roubaud and Lusson 2006</a>; refreshing the page or clicking on "poème" will generate a new poem). <i>JanusNode</i>'s textual "DNA" are similarly syntactic templates that are randomly filled with a given vocabulary.
                  </p>
                  <br><a name="ss1-6-8_t1"></a>
                  <b>
                     
                     </b><p> <b>Table 27 1. </b> Syntactic templates.
                     </p>
                  
                  
                  
                  
                  <table frame="hsides" width="100%">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <tbody valign="top">
                        
                        <tr>
                           
                           <td>Bronze</td>
                           
                           <td>maiden</td>
                           
                           <td>am</td>
                           
                           <td>I</td>
                           
                           <td>and</td>
                           
                           <td>on</td>
                           
                           <td>Midas's</td>
                           
                           <td>mound</td>
                           
                           <td>I</td>
                           
                           <td>lie</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>Silver</td>
                           
                           <td>missy</td>
                           
                           <td></td>
                           
                           <td></td>
                           
                           <td></td>
                           
                           <td>above</td>
                           
                           <td>Goldfinger's</td>
                           
                           <td>heap</td>
                           
                           <td></td>
                           
                           <td>repose</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>Gold</td>
                           
                           <td>miss</td>
                           
                           <td></td>
                           
                           <td></td>
                           
                           <td></td>
                           
                           <td>outside</td>
                           
                           <td>Buster's</td>
                           
                           <td>pile</td>
                           
                           <td></td>
                           
                           <td>perch</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>Metal</td>
                           
                           <td>girl</td>
                           
                           <td></td>
                           
                           <td></td>
                           
                           <td></td>
                           
                           <td>supra</td>
                           
                           <td>Bambi's</td>
                           
                           <td>stack</td>
                           
                           <td></td>
                           
                           <td>recline</td>
                           
                        </tr>
                        
                     </tbody>
                     
                  </table>
                  
                  
                  
                  <br>
                  
                  <p>Formally, such chimeric poetry is possible because it is an easy step to abstract away the syntactic structure of a text and
                     then combine the resulting template with a lexicon. Most of the text's linguistic constraints are managed in the templates
                     (though here too there are some stylistic influences); lexical choice, on the other hand, reflects stylistic and referential
                     constraints which are informed by the author's discursive practices and the world the text describes. Roubaud and Lusson's
                     software automates the combining of templates and lexicons (no doubt the method used in the Malvole (<a href="#ss1-6-8_b34">Round 2006</a>) and <a href="#ss1-6-8_b18">Kurzweil (2006)</a> generators as well, with probabilistic improvements in the latter), but it is clear that template construction and lexicon
                     building could be easily automated as well. From the corpora of two poets, it is possible to generate automatically the two
                     lexicons and templates. What is more challenging is selecting semantically <b>relevant</b> combinations from all the possibilities. A semantic templating system requires a different set of linguistic resources.
                  </p>
                  
                  <a name="ss1-6-8_h5"></a><h3 class="normal">Semantic templates</h3>
                  
                  <p>A semantic template is a systematic lexical choice that reflects either the state of an imaginary world or a particular way
                     to express statements about that world. For instance, from "She likes pink dresses," where the semantic marker "feminine" is repeated in many of the lexical items, we may wish to generate a second, gender-bent sentence "He likes blue pants," where the semantic markers "masculine" systematically replace (as far as possible) the feminine markers. Both sentences, as well as many others such as "It likes tall trees," correspond to the general syntactic template "&lt;pronoun&gt; likes &lt;adjective&gt; &lt;noun&gt;." But we do not want such a large range in meaning; we would like to modulate sentence generation more finely by systematically
                     choosing semantically related words. That requires considerably more information about the relation between lexical items
                     than do the Rimbaudelaire poems.
                  </p>
                  
                  <p>To illustrate some of the difficulties and possibilities of semantic templating, we will explore here the programming that
                     would allow us to move from Midas's epitaph to another version which is considerably more abstract in its expression (<a href="#ss1-6-8_t2">Table 27.2</a>).
                  </p>
                  
                  <p>These automatic transformations replace nouns and verbs (marked with asterisks) with words that are doubly more abstract than
                     the target word. Adjectives (and past participles used as adjectives; both marked with a plus sign) are replaced with words
                     that are similar, but not necessarily more abstract. The procedure requires (1) tagging all the words with their part of speech
                     and then (2) making a programmed walk in the <i>WordNet</i> dictionary that leads from one word to another. This set of programmed walks in <i>WordNet</i> is a semantic template in the sense that there is an overall logic in the shift in meaning that affects the entire text;
                     here, the paradigmatic choice at each syntactic spot in the template is under the same "abstracting" influence. Any given generated text is therefore the product of a semantic template, where word choice is regulated, and
                     a syntactic template, where syntactic agreement is established.
                  </p>
                  <br><a name="ss1-6-8_t2"></a>
                  <b>
                     
                     </b><p> <b>Table 27 .2 </b> Double abstraction of Midas's epitaph.
                     </p>
                  
                  
                  
                  
                  <table frame="hsides" width="100%">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <tbody valign="top">
                        
                        <tr>
                           
                           <td>*Bronze *maiden am I and on Midas's *mound</td>
                           
                           <td>*Alloy *young female that I am and on Midas's</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td> I *lie.</td>
                           
                           <td> *artifact I *be</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>As + long as *water *flows and +tall *trees</td>
                           
                           <td>As long as *fluid *covers and + long-stalked</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td> *bloom,</td>
                           
                           <td> *tracheophytes *grow,</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>Right here + fixed fast on the tearful *tomb,</td>
                           
                           <td>Right here + immobile fast on the + sniffly *point,</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>I shall *announce to all who *pass near: Midas is</td>
                           
                           <td>I shall *inform to all who *change near: Midas is</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td> + dead and + buried here.</td>
                           
                           <td>  +defunct and + belowground here.</td>
                           
                           <td></td>
                           
                        </tr>
                        
                     </tbody>
                     
                  </table>
                  
                  
                  
                  <br>
                  
                  
                  <a name="ss1-6-8_h6"></a><h3 class="normal">Automatic syntactic template building through POS tagging</h3>
                  
                  <p>There are a growing number of public domain packages for creating and manipulating text and its linguistic infrastructure.
                     Text generation systems (e.g., <i>KMPL</i> (<a href="#ss1-6-8_b30">Reiter 2002</a>) and <i>Vinci</i> (Lessard and Levison, described in <a href="#ss1-6-8_b42">Winder 2004</a>: 462ff)) allow for a detailed specification of text structure. Part-of-speech taggers (e.g., <i>TreeTagger</i> (<a href="#ss1-6-8_b38">Stein 2006</a>)), have become fairly robust and simple to operate. General text engineering systems (e.g., <i>OpenNLP</i> (<a href="#ss1-6-8_b4">Baldridge and Morton 2006</a>) and <i>NLTK</i> (Bird et al. n.d.)) are considerably more difficult to master, but accessible even to inexperienced users (with patience!).
                     The pedagogical, well-documented <i>NLTK</i> is built on the humanist-friendly programming language Python. We will use <i>NLTK</i> to tag each word of the text with a part of speech. <i>NLTK</i> has a number of tagging algorithms that can be combined. This is the part-of-speech template that <i>NLTK</i> produces, using several taggers trained on the accompanying Brown corpus:
                  </p>
                  
                  <blockquote>
                     <p>Bronze/jj maiden/nn am/bem I/ppss and/cc on/in Midas/nnp 's/poss mound/nn I/ppss lie/vb ./.</p>
                  </blockquote>
                  
                  <blockquote>
                     <p>As/ql long/rb as/ql water/nn flows/vbz and/cc tall/jj trees/nns bloom/vb, /,</p>
                  </blockquote>
                  
                  <blockquote>
                     <p>Right/ql here/rn fixed/vbn fast/rb on/in the/at tearful/jj tomb/nn</p>
                  </blockquote>
                  
                  <blockquote>
                     <p>I/ppss shall/md announce/vb to/in all/abn who/wps pass/vb near/rp :/:</p>
                  </blockquote>
                  
                  <blockquote>
                     <p>Midas/nnp is/bez dead/jj and/cc buried/vbn here/rb ./.</p>
                  </blockquote>
                  
                  <p>Codes for parts of speech are found in the <i>NLTK</i> documentation;<a href="#ss1-6-8_fn4"><sup>4</sup></a> those that interest us are the adjective (jj), past participle (vbn), noun (nn, nns), and verb (vb). <i>NLTK</i> tagging routines use a set of tagged texts from the Brown Corpus included with <i>NLTK</i> to make a lexicon with parts of speech, frequencies, and POS contexts. Variations in cases ("Bronze" vs. "bronze"), punctuation (all punctuation is tagged as itself here), affix and prefixes ("tearful"/"tear") are analyzed by preprocessors that tokenize the input text. To correctly choose a tag for a given word requires several
                     steps:
                  </p>
                  
                  
                  <p> <b>•</b> The default tagger tags all tokenized words with the proper noun tag (nnp), since most unrecognized words will be in the open
                     class of proper nouns.
                  </p>
                  
                  <p> <b>•</b> Then the unigram tagger finds the most frequent tags in the reference corpus associated with each word and applies them accordingly.
                     "Bronze" will go from nnp to nn, since in the <i>NLTK</i> Brown corpus nn is its most frequent tag for "bronze."
                  </p>
                  
                  <p> <b>•</b> A bigram tagger then looks at what tag <b>pairs</b> are in the database: as it turns out, the Brown corpus has tagged occurrences as both jj+nn and nn+nn: "bronze wreath" is analyzed as "bronze/nn wreath/nn" (vs. "bronze/jj neck/nn"):
                  </p>
                  
                  <p><br><a name="ss1-6-8_fu4"></a>
                     <img src="9781405148641_chapter_27_fu4.gif">
                     <br></p>
                  
                  <p>The bigram tagger understands, however, that the frequency of jj+nn (as in "bronze cannon") is far greater in the corpus than the frequency of nn+nn ("bronze statue") and assumes that since "bronze" is also jj, it can retag it so that the more frequent tag combination is respected. (The tagger is logical, though this is
                     not the standard analysis of "bronze.")
                  </p>
                  
                  
                  <p><i>NLTK</i> includes other taggers (i.e., different algorithms and databases for tagging texts) such as the Brill tagger and Hidden Markov
                     Model tagger that make a more thorough analysis of tag probabilities. For our purposes, the default tagging (with nnp), the
                     unigram tagger, and the bigram tagger are sufficient, in spite of certain errors which we will correct manually. All taggers
                     break down in places, and some sentences are simply not analyzable (to experiment with tagging, see UCREL for the online version
                     of the <i>CLAWS</i> tagger).
                  </p>
                  
                  <p>The basic problem faced by a tagger algorithm is that linguistic information is typically not layered in a homogeneous fashion.
                     For example, though the nn+nn combination is relatively infrequent, in some cases, such as "lawn tennis," it is the only possibility (since "lawn" is not an adjective). Taggers must work not only on a given "horizontal" relation with units at the same level, such as tag combination, but as well they must take into account the idiosyncratic
                     influence of one level on another, such as the influence of a particular lexical choice. Such idiosyncratic "vertical" influences are cited as one justification for tree adjoining grammar formalism, which encapsulate information at different
                     levels using a tree structure — see Abeillé and Rambow 2000: 8.
                  </p>
                  
                  
                  <a name="ss1-6-8_h7"></a><h3 class="normal">Automatic semantic template building: navigating between senses</h3>
                  
                  <p>The POS tagged text is the syntactic half of generation. A semantic template is the complementary organization of the word
                     meanings that are chosen to fill the slots in the syntactic template. Words and their meanings have an intrinsic organization
                     that does not concern syntax at all. <i>WordNet</i> (see <a href="#ss1-6-8_b15">Fellbaum 1998</a>) groups words of like meaning into synsets, i.e., into "a set of words that are interchangeable in some context without changing the truth value of the proposition in which they
                     are embedded" (<a href="#ss1-6-8_b24">Miller 2006</a><a href="#ss1-6-8_fn5"><sup>5</sup></a>). Synsets are themselves related according to several semantic relations, among which: hypernymy/hyponymy (the more general
                     type of the source word; e.g., "color" is the hypernym of "red"; inversely, "red" is the hyponym of "color"); holonymy/meronymy (the whole of which the source word is a part; e.g., "crowd" is the holonym of "person"); antonyms (extreme opposites such as "good" and "bad"), similar to (adjectives that mean something similar; e.g., "immobile" is similar to "fixed"), and entailment (the source word has as a logical consequence another: e.g., "snoring" entails "sleeping").
                  </p>
                  
                  <p>Just as our syntactic template is built out of the general syntactic categories (like "noun" and "verb"), our semantic template will use general relations, like "antonym" and "similar to", to describe in general terms the meaning structures of a generated sentence.</p>
                  
                  
                  <a name="ss1-6-8_h8"></a><h3 class="normal">Double hypernyms</h3>
                  
                  <p>Each word of our epitaph has a position in the <i>WordNet</i> meaning universe which is defined by its synset membership. The word "bloom" belongs to 6 noun synsets and 1 verb synset:
                  </p>
                  
                  <p><b>Noun</b></p>
                  
                  
                  <p> <b>•</b> S: (n) blooming, <b>bloom</b> (the organic process of bearing flowers) <i>"you will stop all bloom if you let the flowers go to seed"</i></p>
                  
                  <p> <b>•</b> S: (n) flower, <b>bloom</b>, blossom (reproductive organ of angiosperm plants especially one having showy or colorful parts)
                  </p>
                  
                  <p> <b>•</b> S: (n) <b>bloom</b>, bloom of youth, salad days (the best time of youth)
                  </p>
                  
                  <p> <b>•</b> S: (n) <b>bloom</b>, blush, flush, rosiness (a rosy color (especially in the cheeks) taken as a sign of good health)
                  </p>
                  
                  <p> <b>•</b> S: (n) flower, prime, peak, heyday, <b>bloom</b>, blossom, efflorescence, flush (the period of greatest prosperity or productivity)
                  </p>
                  
                  <p> <b>•</b> S: (n) efflorescence, <b>bloom</b> (a powdery deposit on a surface)
                  </p>
                  
                  
                  <p><b>Verb</b></p>
                  
                  
                  <p> <b>•</b> S: (v) <b>bloom</b>, blossom, flower (produce or yield flowers) <i>"The cherry tree bloomed"</i> ("bloom" in Miller <i>WordNet</i><a href="#ss1-6-8_fn6"><sup>6</sup></a>)
                  </p>
                  
                  
                  <p>The synset for the verb "bloom" contains "bloom, blossom, flower". The lexical relations that that synset entertains with other synsets can be displayed by clicking on the synset ("S"). The first-level direct hypernym of the verb "bloom" is thus "develop" (a blooming is a kind of developing) and the hypernym of "develop" is "grow" (a developing is a kind of growing):</p>
                  
                  
                  <p> <b>•</b> S: (v) <b>bloom</b>, blossom, flower (produce or yield flowers) <i>"The cherry tree bloomed"</i></p>
                  
                  <p> <b> •</b> <i>direct hypernym</i></p>
                  
                  <p> <b>  •</b> S: (v) develop (grow, progress, unfold, or evolve through a process of evolution, natural growth, differentiation, or a conducive environment)
                     <i>"A flower developed on the branch"; "The country developed into a mighty superpower"; "The embryo develops into a fetus"; "This situation has developed over a long time"</i></p>
                  
                  <p> <b>   •</b> <i>direct hypernym</i></p>
                  
                  <p> <b>    •</b> S: (v) grow (become larger, greater, or bigger; expand or gain) <i>"The problem grew too large for me"; "Her business grew fast"</i> ("bloom" in <a href="#ss1-6-8_b24">Miller 2006</a><a href="#ss1-6-8_fn7"><sup>7</sup></a>)
                  </p>
                  
                  
                  <p>The abstracting algorithm will take all the nouns and verbs in our epitaph and navigate to their second-level direct hypernyms
                     (a double hypernym) and put that double hypernym (or the first word of the synset, if the synset has more than one word) in
                     place of the original word of the epitaph. In a similar way, adjectives and past participles will be replaced with "similar to" words. In the case of the verb "bloom," the replacement word is "grow."
                  </p>
                  
                  
                  <a name="ss1-6-8_h9"></a><h3 class="normal">Word sense disambiguation</h3>
                  
                  <p><i>WordNet</i> comes with an assortment of tools for navigating the database (in many programming languages, but the Python modules are
                     perhaps the most compatible with <i>NLTK</i>). In our double hypernym navigation we will generally have to choose between different senses, just as when we syntactically
                     parsed the text we had to decide which part of speech was appropriate. In the case of navigating the sense tree of "bloom", there is no confusion; only one sense is under verb and only one hypernym at each level (bloom → develop → grow). The adjective "dead", on the other hand, has 21 senses (i.e., it belongs to 21 different synsets). We need a method for disambiguating different
                     meanings.
                  </p>
                  
                  <p>Sense disambiguation depends on context, both syntactic and semantic. For example, to disambiguate "lie" it is useful to know that the subject is a person; in our text, "I" is a personified object, so "I" can tell a lie, lie down, but rarely will "I" lie in a certain position, as in "the mountains lie in the west." Semantic clues also come from the set of words that are in the same passage, whatever their syntactic relation. "Mound" could be a baseball mound, but since in the same context we find "tomb," "dead," and "tearful," our choice would favor a burial mound (though that sense is not found in <i>WordNet</i>).
                  </p>
                  
                  <p><i>SenseRelate</i> (Pedersen) is a Perl package that attempts to compare all such relations and disambiguate words according to their immediate
                     context, their position in the <i>WordNet</i> ontology, and a corpus of sense tagged texts. <i>SenseRelate</i>, properly configured, will semantically tag our text as follows (<i>WordNet</i> only tags nouns, adverbs, adjectives and verbs) (<a href="#ss1-6-8_t3">Table 27.3</a>).
                  </p>
                  
                  <p>The online version of <i>SenseRelate</i><a href="#ss1-6-8_fn8"><sup>8</sup></a> shows how meanings are selected according to several algorithms that quantify how semantically close entries are in the <i>WordNet</i> network. The Lesk algorithm, for example, evaluates word overlaps in the glosses for each meaning. As we can see above, "tomb" and "mound" are related by the words "<b>earth</b>" and "<b>stone</b>" (in the gloss of "mound") and "ground" and "tomb<b>stone</b>" (in "tomb"; "ground" has for gloss "the solid part of the <b>earth's</b> surface"); "dead" and "tomb" are linked by the relation between "life" and "<b>dead man</b>" (in "dead") and "corpse" (in "tomb"; the gloss of "corpse" is "the <b>dead</b> body of a <b>human</b> being"). <a href="#ss1-6-8_t4">Table 27.4</a> shows the Lesk values for the latter pair (top values only; the rest are 0), as generated by the online tool; sense 1 of
                     the adjective "dead" is most closely related to sense 1 of the noun "tomb".
                  </p>
                  <br><a name="ss1-6-8_t3"></a>
                  <b>
                     
                     </b><p> <b>Table 27 .3 </b> SenseRelate meaning selection (excerpt).
                     </p>
                  
                  
                  
                  
                  <table frame="hsides" width="100%">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <thead valign="bottom">
                        
                        <tr>
                           
                           <th>Word POS Sense</th>
                           
                           <th>WordNet Gloss</th>
                           
                        </tr>
                        
                     </thead>
                     
                     <tbody valign="top">
                        
                        <tr>
                           
                           <td>BRONZE n 1</td>
                           
                           <td>an alloy of copper and tin and sometimes other elements; also any copper-base alloy containing other elements in place of
                              tin
                           </td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>MAIDEN n 1</td>
                           
                           <td>an unmarried girl (especially a virgin) …</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>MIDAS n 1</td>
                           
                           <td>(Greek legend) the greedy king of Phrygia who Dionysus gave the power to turn everything he touched into gold</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>
<b>MOUND</b> n 4
                           </td>
                           
                           <td>a structure consisting of an artificial heap or bank usually of <b>earth</b> or <b>stones</b>; "they built small mounds to hide behind" …
                           </td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>TEARFUL a 1</td>
                           
                           <td>filled with or marked by tears; "tearful eyes"; "tearful entreaties"</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>
<b>TOMB</b> n 1
                           </td>
                           
                           <td>a place for the burial of a <b>corpse</b> (especially beneath the <b>ground</b> and marked by a tombstone); "he put flowers on his mother's grave" …
                           </td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>
<b>DEAD</b> a 1
                           </td>
                           
                           <td>no longer having or seeming to have or expecting to have <b>life</b>; "the nerve is dead"; "a dead pallor"; "he was marked as a <b>dead man</b> by the assassin" …
                           </td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>BURY v 2</td>
                           
                           <td>place in a grave or tomb; "Stalin was buried behind the Kremlin wall on Red Square"; "The pharaohs were entombed in the pyramids"; "My grandfather was laid to rest last Sunday"</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>HERE r 1</td>
                           
                           <td>in or at this place; where the speaker or writer is; "I work here"; "turn here"; "radio waves received here on Earth"</td>
                           
                        </tr>
                        
                     </tbody>
                     
                  </table>
                  
                  
                  
                  <br>
                  <br><a name="ss1-6-8_t4"></a>
                  <b>
                     
                     </b><p> <b>Table 27 .4 </b> <i>SenseRelate</i> word relatedness comparison ("dead" and "tomb").
                     </p>
                  
                  
                  
                  
                  <table frame="hsides" width="100%">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <thead valign="bottom">
                        
                        <tr>
                           
                           <th>Measure</th>
                           
                           <th>Word 1</th>
                           
                           <th>Word 2</th>
                           
                           <th>Score</th>
                           
                        </tr>
                        
                     </thead>
                     
                     <tbody valign="top">
                        
                        <tr>
                           
                           <td>lesk</td>
                           
                           <td>dead#a#1</td>
                           
                           <td>tomb#n#1</td>
                           
                           <td>6</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>lesk</td>
                           
                           <td>dead#n#1</td>
                           
                           <td>tomb#n#1</td>
                           
                           <td>4</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>lesk</td>
                           
                           <td>dead#n#2</td>
                           
                           <td>tomb#n#1</td>
                           
                           <td>3</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>lesk</td>
                           
                           <td>dead#r#2</td>
                           
                           <td>tomb#n#1</td>
                           
                           <td>2</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>lesk</td>
                           
                           <td>dead#a#2</td>
                           
                           <td>tomb#n#1</td>
                           
                           <td>1</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>lesk</td>
                           
                           <td>dead#a#14</td>
                           
                           <td>tomb#n#1</td>
                           
                           <td>1</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>lesk</td>
                           
                           <td>dead#a#3</td>
                           
                           <td>tomb#n#1</td>
                           
                           <td>0</td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>…</td>
                           
                           <td></td>
                           
                           <td></td>
                           
                           <td></td>
                           
                        </tr>
                        
                     </tbody>
                     
                  </table>
                  
                  
                  
                  <br>
                  
                  <p>We use such sense relatedness measures to choose between alternative hypernyms or "similar to" adjectives in our abstracting algorithm.</p>
                  
                  
                  <a name="ss1-6-8_h10"></a><h3 class="normal">Automatic narrative</h3>
                  
                  <p>Narrative represents the third major topography. Most generators do not deal directly with that topography; rather, narrative
                     generation is generally approached in the same manner as syntactic and semantic generation, with a few enhancements. <a href="#ss1-6-8_t5">Table 27.5</a> shows some sample generation from <i>JanusNode</i>.
                  </p>
                  
                  <p>The French site <i>Charabia</i> (see <a href="#ss1-6-8_b31">Reyes 2006</a>) has a generator which subscribers use to produce various texts. Here is the start of a (long) philosophical essay that one
                     user programmed:
                  </p>
                  
                  <blockquote>
                     <p><b>Tribalisme vs tribalisme</b></p>
                  </blockquote>
                  
                  <blockquote>
                     <p>1. Prémisses du tribalisme idéationnel.</p>
                  </blockquote>
                  
                  <blockquote>
                     <p>Si on ne saurait ignorer la critique bergsonienne du primitivisme substantialiste, Montague systématise néanmoins la destructuration primitive du tribalisme et il en identifie, par la même, l'aspect transcendental en tant que concept primitif de la connaissance.</p>
                  </blockquote>
                  
                  <blockquote>
                     <p>Cependant, il identifie le primitivisme de la société tout en essayant de le resituer dans toute sa dimension sociale, et le tribalisme ne se borne pas à être un primitivisme existentiel comme concept rationnel de la connaissance.</p>(Reyes at <i>Charabia</i><a href="#ss1-6-8_fn9"><sup>9</sup></a>)
                  </blockquote>
                  
                  <p>Both these generators script narrative through different kinds of syntactic templating. <i>Charabia</i> uses a simplified transition network (<a href="#ss1-6-8_f1">Figure 27.1</a>).<a href="#ss1-6-8_fn10"><sup>10</sup></a></p>
                  <br><a name="ss1-6-8_t5"></a>
                  <b>
                     
                     </b><p> <b>Table 27 .5 </b> JanusNode sample story plot output.
                     </p>
                  
                  
                  
                  
                  <table frame="hsides" width="100%">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <col align="">
                     
                     <thead valign="bottom">
                        
                        <tr>
                           
                           <th>(none)</th>
                           
                           <th>Possible title: The Hostility<br> of Erroneous Hate
                           </th>
                           
                           <th>Possible title: Baggaging Bros</th>
                           
                        </tr>
                        
                     </thead>
                     
                     <tbody valign="top">
                        
                        <tr>
                           
                           <td>This story has two main characters. The first is Noel, a dogged air traffic controller. The second is named Erna. Erna is
                              a dead angel. This is their story, of sex and solitude.
                           </td>
                           
                           <td>This story has two main characters. The first is a cold master named Una. The second is named Alonso. Alonso is a drunk master.</td>
                           
                           <td>Your protagonist is a brown skin broke bro named Larissa. Your antagonist is a kindhearted agent named Griselda. Their story
                              is one of creativity and open-heartedness.
                           </td>
                           
                        </tr>
                        
                        <tr>
                           
                           <td>Noel and Erna meet in an useful motel. Noel wants to discuss personalizing and hugging. Erna is tired of being dead and needing.
                              She knows that Noel is neither dead nor needing. Noel is only after one thing: contents. Erna needs to get contents. The well-intentioned
                              air traffic controller explains to the thrilling angel that the difficult animation is like an atmosphere. Erna feels confused.
                              The moral is worth remembering. Clairvoyant sex is not the same as optimal solitude.
                           </td>
                           
                           <td>Una and Alonso meet in a boisterous crimson hospital waiting room. Alonso is thinking of giving up being a master to study
                              epitaphs with Una. Una takes out a hostility, and advises Alonso to be more quarrelsome. The penniless master reveals the
                              hostility to the master. Alonso becomes less drunk.
                           </td>
                           
                           <td>Larissa and Griselda meet in an alien furthest sitting room. Larissa and Griselda have been brought together by their common
                              friendship with a third character named Nicola. Griselda is thinking of giving up being an agent in order to pursue an interest
                              in studying intentional metaphysics. Larissa gives benumbed information about the importance of weakness and destruction.
                              The illiterate agent steals the baggage from the brown skin bro. Larissa becomes more brown skin. The moral is instructive.
                              Interesting creativity is only xenophobic open-heartedness.
                           </td>
                           
                        </tr>
                        
                     </tbody>
                     
                  </table>
                  
                  
                  
                  <br>
                  
                  <p>Generation passes through each node in a random fashion, text is written out, variables are set that select the lexicon ($theme_dtd qualifies the chosen thematic word, $theme), loops are defined, and control is passed to subnetworks (comp2, a complement such as "au regard du maximalisme", and adj1_fs, a feminine singular adjective, are described in other networks).</p>
                  
                  <p><i>JanusNode</i> uses the same techniques for the most part, but the network is implemented with scripts called TextDNA:
                  </p>
                  
                  <blockquote>
                     <p>100 Subject(AnimalPoemSetUp) &lt; assign(Cur,"dog,cat,pig") 100 &gt;</p>
                     
                     <p>100 Subject(ShortAnimalPoem) "A" 100 &lt; GetRhyme(Cur,noun) 100 &gt;</p>
                     
                     <p>"likes to" 100 &lt; GetRhyme(Cur,verb) 100</p>
                     
                     <p>&gt; "!" 100</p>
                  </blockquote>
                  
                  <blockquote>
                     <p>A pig likes to jig!</p>
                     
                     <p>A cat likes to bat!</p>
                     
                     <p>(TextDNA and sample output from <i>JanusNode</i> documentation)
                     </p>
                  </blockquote>
                  
                  <p>The numbers indicate the probability that a given function will fire (100 means that the function will fire 100% of the time);
                     the <i>assign</i> function chooses a word randomly in a set or file of words given in the second argument and assigns it to the variable given
                     in the first argument. <i>JanusNode</i> has some built-in linguistic functionality: a rhyming function (GetRhyme) and minimal morphology. It also has a rudimentary
                     TextDNA generation system which will make scripts from source text. Properly configured, <i>JanusNode</i> scripts are more powerful than <i>Charabia</i>'s networks, but are more tedious to write.
                  </p>
                  
                  <p><br><a name="ss1-6-8_f1"></a>
                     <img src="9781405148641_chapter_27_f1.gif">
                     <b>
                        
                        </b></p>
<p> <b>Figure 27.1 </b> Intro1 node of the <i>Charabia</i> generator.
                        </p>
                     <br>
                  
                  <p>One significant variation on this basic template framework for generation are systems that are designed for dialogue, such
                     as <i>A.L.I.C.E</i>. (and the <i>AIML</i> scripting language). Narrative is developed "on the fly" in response to the user's input. In such cases the input text is transformed into data for generation.
                  </p>
                  
                  
                  <a name="ss1-6-8_h11"></a><h3 class="normal">Narrative topology: chaining events</h3>
                  
                  <p>Generating truly creative narrative poses a particular problem. The template systems we have dealt with so far only concern
                     the most combinable units of language —words — through word-centered syntax (POS tagging) and semantics (dictionary meanings). Programming narrative is difficult because
                     the fundamental topography of narrative is not linguistic, but rather a topography of events. <b>Grammars</b> offer much accumulated knowledge about syntax that is formalized; <b>dictionaries</b> describe word meaning in a very clear fashion; <b>encyclopedias</b> describe events, but with almost no formalization. No convincing narrative can be made without a large database of our commonsense
                     understanding of events. Narrative does have a linguistic component as well, which dictates how information will be parceled
                     out to the reader. For instance, it is a purely linguistic fact that the order of sentences typically represents the order
                     of events. We understand differently "Paul broke down in tears. Mary broke his channel changer" and "Mary broke his channel changer. Paul broke down in tears." However, it is a fact of the world, not of language, that the destruction of an object might have an emotional impact on
                     a person. Neither a grammar nor a dictionary has that fundamental knowledge, knowledge that a story generator must inevitably
                     orchestrate.
                  </p>
                  
                  <p>The lightweight templating systems we have seen so far require the user to build events through language. On the contrary,
                     an ideal story generator would either generate events automatically, selected from an events database, or take a general description
                     of an event and generate the appropriate language. <b>Good</b> narrative, the most general topography and the one that connects language most directly to reality, is particularly difficult
                     to master because it is at this level that all the underlying topologies are given a final meaning. Narrative is where information
                     at different levels is <b>evaluated</b>. Poetry shows this most clearly, since even the simple sound or spelling of a word might be the focus of the text. Thus a
                     story generator about birds in French might need to know something about the character level of the word "oiseau" (that it has all the vowels). Similarly, in our epitaph it is perhaps important, or not, that the maiden be bronze, rather
                     than silver. Ultimately, the pertinence of a lexical choice can only be determined by the goal of the narrative; narrative
                     establishes a general logic and hierarchy for all the other kinds of information in the text.
                  </p>
                  
                  <p>Heavyweight narrative generators (see <a href="#ss1-6-8_b27">Mueller 2006</a> for a bibliography and resources) depend on large databases of encyclopedic knowledge. <i>MIT</i>'s <i>Open Mind Common Sense</i> (<i>OMCS</i>; see <a href="#ss1-6-8_b37">Singh et al. 2002</a>) was conceived to collect a database of common sense reasoning from internet users and distill it into a computer-usable
                     form. General event scripts are added to these databases or generated from the encyclopedic knowledge. <i>Thought Treasure</i> (Mueller 2000<a href="#ss1-6-8_fn11"><sup>11</sup></a>) is a well-documented, pioneering reasoning database that was used as a model for the <i>OMCS</i> database. Here is one of the handcrafted scripts from <i>Thought Treasure</i>:
                  </p>
                  
                  
                  <p>2. (sleep) [frequent] sleep; [English] sleep; [French] dormir</p>
                  
                  <p>[ako <sup>∧</sup> personal-script]
                  </p>
                  
                  <p>[cost-of <sup>∧</sup> NUMBER:USD:0]
                  </p>
                  
                  <p>[duration-of <sup>∧</sup> NUMBER:second:28800]
                  </p>
                  
                  <p>[entry-condition-of <sup>∧</sup> [sleepiness sleeper]]
                  </p>
                  
                  <p>[event01-of <sup>∧</sup> [strip sleeper]]
                  </p>
                  
                  <p>[event02-of <sup>∧</sup> [ptrans-walk sleeper na bed]]
                  </p>
                  
                  <p>event03-of <sup>∧</sup> [set sleeper alarm-clock]]
                  </p>
                  
                  <p>event04-of <sup>∧</sup> [lie-on sleeper bed]]
                  </p>
                  
                  <p>event05-of <sup>∧</sup> [groggy sleeper]]
                  </p>
                  
                  <p>event06-of <sup>∧</sup> [sleep-onset sleeper]]
                  </p>
                  
                  <p>event07-of <sup>∧</sup> [asleep sleeper]]
                  </p>
                  
                  <p>event07-of <sup>∧</sup> [dream sleeper]]
                  </p>
                  
                  <p>event08-of <sup>∧</sup> [ring alarm-clock]]
                  </p>
                  
                  <p>event08-of <sup>∧</sup> [wake alarm-clock sleeper]]
                  </p>
                  
                  <p>event09-of <sup>∧</sup> [awake sleeper]]
                  </p>
                  
                  <p>event20-of <sup>∧</sup> [rise-from sleeper bed]]
                  </p>
                  
                  <p>goal-of <sup>∧</sup> [s-sleep sleeper]]
                  </p>
                  
                  <p>performed-in <sup>∧</sup> bedroom]
                  </p>
                  
                  <p>period-of <sup>∧</sup> NUMBER:second:86400]
                  </p>
                  
                  <p>result-of <sup>∧</sup> [restedness sleeper]]
                  </p>
                  
                  <p>role01-of <sup>∧</sup> sleeper]
                  </p>
                  
                  <p>role02-of <sup>∧</sup> bed]
                  </p>
                  
                  <p>role03-of <sup>∧</sup> alarm-clock]
                  </p>
                  
                  <p>   (<a href="#ss1-6-8_b25">Mueller 1999</a>)
                  </p>
                  
                  
                  <p>The sleep script describes the event as being personal, costing nothing (in US dollars!) and having a duration of 28,800 seconds
                     (the conventional 8 hours), requiring a sleeper and sleepiness, having a series of subevents, such as stripping, the sleeper
                     walking (from n/a) to bed, setting the alarm clock, lying on the bed, feeling groggy, beginning to go to sleep, being asleep
                     and dreaming, the alarm clock ringing, etc.
                  </p>
                  
                  <p>This handcrafted script gives the barest understanding of the sleep event. The <i>Open Mind</i> project collected 700,000 commonsense statements (the <i>OMCS</i> raw data) about the human experience, deduced some 1.7 million formal relations, and compiled them in the <i>ConceptNet</i> application (Lui et al. 2006). Over 2,000 sentences of the <i>OMCS</i> raw data use the word "sleep" or its direct derivatives; here is an arbitrary sample:
                  </p>
                  
                  
                  <p> <b>1. </b> If a person is bored he may fall a<b>sleep</b></p>
                  
                  <p> <b>2. </b> Jake has two states — awake and a<b>sleep</b></p>
                  
                  <p> <b>3. </b> You are likely to find a cat in front of a fireplace, <b>sleep</b>ing
                  </p>
                  
                  <p> <b>4. </b> Sometimes viewing a film at home causes you to fall a<b>sleep</b> on the sofa
                  </p>
                  
                  <p> <b>5. </b> A motel is a place where you can rent a room to <b>sleep</b> in
                  </p>
                  
                  <p> <b>6. </b> Many people have schedules that allow them to <b>sleep</b> later on weekends than on weekdays
                  </p>
                  
                  <p> <b>7. </b> The effect of attending a classical concert is falling a<b>sleep</b></p>
                  
                  <p> <b>8. </b> Something you might do while watching a movie is falling a<b>sleep</b></p>
                  
                  <p> <b>9. </b> A sofa hide-a-bed is for <b>sleep</b>ing
                  </p>
                  
                  <p> <b>10. </b> Sometimes taking final exams causes <b>sleep</b>iness
                  </p>
                  
                  <p> <b>11. </b> Ken fell a<b>sleep</b></p>
                  
                  <p> <b>12. </b> Studies have shown that <b>sleep</b> deprivation leads to impaired consolidation of both declarative and procedural memories
                  </p>
                  
                  <p> <b>13. </b> Something you might do while <b>sleep</b>ing is fall out of bed
                  </p>
                  
                  <p> <b>14. </b> An activity someone can do is <b>sleep</b> in a hotel
                  </p>
                  
                  <p> <b>15. </b> You would relax because you want to go to <b>sleep</b></p>
                  
                  <p> <b>16. </b> Cindy Lou has a home to <b>sleep</b> in
                  </p>
                  
                  <p> <b>17. </b> You would <b>sleep</b> at night because you had a busy day
                  </p>
                  
                  <p>(<i>OMCS</i> raw data)
                  </p>
                  
                  
                  <p>Heavyweight story generators (such as <i>Make Believe</i> (<a href="#ss1-6-8_b20">Liu and Singh 2002</a>); for others see Meuller 2006) will use such information to create a narrative topography. A character sleeping in bed might
                     be in her home, and she may fall out, dream, snore, it could be at the end of a busy day, etc. Simply ordering a selection
                     of the sentences given above and making adjustments gives a rough narrative sequence describing Jake's falling asleep and
                     then falling out of bed (words inserted are in bold; words deleted are struck out):
                  </p>
                  
                  
                  <p> <b>1. </b> Jake has two states — awake and asleep
                  </p>
                  
                  <p> <b>17. </b> You <b>too</b> would sleep at night because <b>if</b> you had a busy day
                  </p>
                  
                  <p> <b>15. </b> You would relax because you want to go to sleep
                  </p>
                  
                  <p> <b>4. </b> Sometimes viewing a film at home causes you to fall asleep on the sofa
                  </p>
                  
                  <p> <b>9. </b> A sofa hide-a-bed is for sleeping
                  </p>
                  
                  <p> <b>11. </b> Ken <b>Jake</b> fell asleep <b>on the hide-a-bed</b></p>
                  
                  <p> <b>13. </b> Something you <b>too</b> might do while sleeping is fall out of bed
                  </p>
                  
                  <p>…</p>
                  
                  
                  <p><i>ConceptNet</i>'s database is a network of the abstract rendering of such messy statements about the world. Every node of the network will
                     have "incoming" and "outgoing" relations with other concepts (<a href="#ss1-6-8_f2">Figure 27.2</a>).
                  </p>
                  
                  <p>Automatic generation with <i>ConceptNet</i> consists in following event links, playing out the combinatory of their subevents, and creatively developing descriptions
                     based on the other types of relations in the network.
                  </p>
                  
                  
                  
                  <a name="ss1-6-8_h12"></a><h2 class="normal">Art</h2>
                  
                  <p>The techniques described here do not produce very comprehensible nor artistic narrative. There is little chance that computers
                     will figure soon on the list of best selling authors. At the same time, these techniques are prerequisites for creative writing
                     by computers or computer-assisted writing by people. Text processors will soon offer thesaurus-like functions that produce
                     variants of a sentence, a paragraph, or perhaps even a text. But what makes <b>good</b> writing?
                  </p>
                  
                  <p><br><a name="ss1-6-8_f2"></a>
                     <img src="9781405148641_chapter_27_f2.gif">
                     <b>
                        
                        </b></p>
<p> <b>Figure 27.1 </b> <i>ConceptNet's</i> network of concepts (Liu and Singh).
                        </p>
                     <br>
                  
                  <p>According to one successful writer, Stephen King, good writing is not about the mechanics of plot:</p>
                  
                  <blockquote>
                     <p>Stories are found things, like fossils in the ground […]. Stories aren't souvenir tee-shirts or <i>GameBoys</i>. Stories are relics, part of an undiscovered pre-existing world. The writer's job is to use the tools in his or her toolbox
                        to get as much of each one out of the ground as intact as possible. Sometimes the fossil you uncover is small; a seashell.
                        Sometimes it's enormous, a <i>Tyrannosaurus Rex</i> with all those gigantic ribs and grinning teeth. Either way, short story or thousand-page whopper of a novel, the techniques
                        of excavation remain basically the same.
                     </p>(2000: 163)
                  </blockquote>
                  
                  <p>Getting a fossil out of the ground requires delicate tools, like a palm-pick, airhose, or toothbrush. Plot is the jackhammer
                     of writers: "a good writer's last resort and the dullard's first choice" (2000: 164). As an exercise for the aspiring writer King describes (in his own good style) a bare-bones narrative about a
                     woman who is stalked in her home by her estranged husband:
                  </p>
                  
                  <blockquote>
                     <p>It's a pretty good story, yes? I think so, but not exactly unique. As I've already pointed out, estranged hubby beats up (or murders) ex-wife makes the paper every other week, sad but true. What I want you to do in this exercise is change the sexes of the antagonist
                        and the protagonist before beginning to work out the situation in your narrative […]. Narrate this without plotting — let the situation and that one unexpected inversion carry you along. I predict you will succeed swimmingly… if, that is, you are honest about how your characters speak and behave. Honesty in story telling makes up for a great many
                        stylistic faults, as the work of wooden-prose writers like Theodore Dreiser and Ayn Rand shows, but lying is the great unrepairable
                        fault.
                     </p>(2000: 173)
                  </blockquote>
                  
                  <p>The great gap between printing and writing seems indeed to concern that "honesty in story telling"; writers must use the finest brush to extract from the mass of our beliefs about the world a single compelling image of the
                     way things inescapably are. But how can the writer be honest about <b>fictional</b> characters!? King's honesty is about what our understanding of the world will allow us to put together reasonably (in his exercise, it
                     is understanding the difference between the sexes in their manner of stalking). How things cohere <b>in principle</b> is a truth that writer and reader must both possess in order to understand each other. That coherence is no more nor less
                     than who writer and reader are together, as one being. It defines the single honest way to set about thinking and speaking.
                  </p>
                  
                  <p>Let us assume, then, that the artistic text captures the higher truth of the way things are. It is priceless, lively, and
                     live because, by being intensely, exactly itself, it subsumes the many variants of itself. A gender-bent text is artistic
                     only when it implicitly says the straight text and underlines the inescapable reasoning that brings both texts under a same
                     denomination and so subsumes by the same stroke the many possible variations on the theme. By showing what stalking <b>might</b> be, it shows what stalking <b>must</b> be, however the situation may vary. Artistic texts, clearly departing from the norm by a twist or a wriggle, scintillate
                     with meaning and show dramatically the principle of how they came to be — their "makedness." The deepest truth about anything is the way it comes to be, because there lies the secret of how it might continue to be.
                  </p>
                  
                  <p>If poetic art is indeed "impeded form" or "roughened language" (Shklovsky), it is because narrative leads the reader to tease out a synthetic position — a higher ground —that explains the world of perplexing opposites. Artistic narrative is "roughened reasoning" that leads up to a communal, true way to seeing as one the perplexing opposites of the world. Socrates's complaint about
                     Midas's epitaph is that it does not go anywhere: there is no excavation project to exhume the truth of Midas. There is only
                     the clanking tongue of a bronze maiden, just as petrified as Midas in his grave. Combinatory is not art. Narrative goes somewhere
                     specific: up.
                  </p>
                  
                  <p>Can computers really write? Only if they can fly. They must move up out of their clanking linguistic machinery to a general truth about the world and
                     to a vantage point that captures the text's fundamental generativity. A <b>good</b> text — an artistic text —is the one that represents best many other texts.
                  </p>
                  
                  <p>Remarkable steps have been made to give machines the resources needed to build higher meaning, but it will take still more
                     accumulation of data about how people see the world. Minsky estimates that even simple commonsense "is knowing maybe 30 or 60 million things about the world and having them represented so that when something happens, you can
                     make analogies with others" (<a href="#ss1-6-8_b13">Dreifus 1998</a>, cited in Liu and Singh forthcoming: 2). We will only see computers generate art from all that commonsense when they can
                     be programmed to tell us something true.
                  </p>
                  
                  
                  <a name="ss1-6-8_h13"></a><h2 class="normal">Notes</h2>
                  
                  <p> <b>1 </b> &lt;<a href="http://www.poignantguide.net/ruby/chapter-3.html">http://www.poignantguide.net/ruby/chapter-3.html</a>&gt;, section 1. "Language and I MEAN Language."
                  </p>
                  
                  <p> <b>2 </b> &lt;<a href="http://www.perseus.org/cgi-bin/ptext?doc=Perseus:text:1999.01.0173:text=Phaedrus:section=264d">http://www.perseus.org/cgi-bin/ptext doc Perseus:text:1999.01.0173:text Phaedrus:section 264d</a>&gt; and "Greek display."
                  </p>
                  
                  <p> <b>3 </b> &lt;<a href="http://www.perseus.org/cgi-bin/ptext?doc=Perseus:text:1999.01.0173:text=Phaedrus:section=264d">http://www.perseus.org/cgi-bin/ptext doc Perseus:text:1999.01.0173:text Phaedrus:section 264d</a>&gt; and "Greek transliterated display."
                  </p>
                  
                  <p> <b>4 </b> &lt;<a href="http://nltk.sourceforge.net/tutorial/tagging/section-a1189.html">http://nltk.sourceforge.net/tutorial/tagging/section-a1189.html</a>&gt;.
                  </p>
                  
                  <p> <b>5 </b> &lt;<a href="http://wordnet.princeton.edu/man/wngloss.7WN">http://wordnet.princeton.edu/man/wngloss.7WN</a>&gt;.
                  </p>
                  
                  <p> <b>6 </b> &lt;<a href="http://wordnet.princeton.edu/perl/webwn?o2=&amp;o0=1&amp;o7=&amp;o5=&amp;o1=1&amp;o6=&amp;o4=&amp;o3=&amp;s=bloom">http://wordnet.princeton.edu/perl/webwn o2 &amp;o0 1&amp;o7 &amp;o5 &amp;o1 1&amp;o6 &amp;o4 &amp;o3 &amp;s bloom</a>&gt;.
                  </p>
                  
                  <p> <b>7 </b> &lt;<a href="http://wordnet.princeton.edu/perl/webwn?o2=&amp;o0=1&amp;o7=&amp;o5=&amp;o1=1&amp;o6=&amp;o4=&amp;o3=&amp;s=bloom&amp;i=12&amp;h=00000010110000000#c">http://wordnet.princeton.edu/perl/webwn o2 &amp;o0 1&amp;o7 &amp;o5 &amp;o1 1&amp;o6 &amp;o4 &amp;o3 &amp;s bloom&amp;i 12&amp;h 00000010110000000#c</a>&gt;.
                  </p>
                  
                  <p> <b>8 </b> &lt;<a href="http://marimba.d.umn.edu/cgi-bin/similarity.cgi">http://marimba.d.umn.edu/cgi-bin/similarity.cgi</a>&gt;.
                  </p>
                  
                  <p> <b>9 </b> &lt;<a href="http://www.charabia.net/gen/gendisp.php?gen=1/&amp;big=1">http://www.charabia.net/gen/gendisp.php gen 1 &amp;big 1</a>&gt;. 
                  </p>
                  
                  <p> <b>10 </b> &lt;<a href="http://www.charabia.net/gen/gendisp.php?gen=1/&amp;big=1/&amp;fonc=1">http://www.charabia.net/gen/gendisp.php gen 1 &amp;big 1 &amp;fonc 1</a>&gt;.
                  </p>
                  
                  <p> <b>11 </b> &lt;<a href="http://www.signiform.com/tt/python/query.cgi">http://www.signiform.com/tt/python/query.cgi</a>&gt;.
                  </p>
                  
                  
                  
                  
                  <a name="ss1-6-8_h14"></a><h2 class="normal">Bibliography</h2>
                  <a name="ss1-6-8_b1"></a><p class="hang">Abeillé, A. and O. Rambow (2000). "<i><span class="title">Tree Adjoining Grammar: An Overview.</span></i>" In A. Abeillé and O. Rambow (Eds.). <i><span class="title">Tree Adjoining Grammars: Formalism, Linguistic Analysis and Processing</span></i>. Stanford: CSLI Publications, pp. 1–68. 
                  </p>
                  <a name="ss1-6-8_b2"></a><p class="hang">A. L. I. C. E. AI Foundation (2006). &lt;<a href="http://www.alicebot.org/">http://www.alicebot.org/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b3"></a><p class="hang"><i><span class="title">Babel Fish Translation</span></i> (2006). &lt;<a href="http://babelfish.altavista.com/">http://babelfish.altavista.com/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b4"></a><p class="hang">Baldridge, Jason and Tom Morton (Eds.) (2006). <i><span class="title">OpenNLP</span></i>. &lt;<a href="http://opennlp.sourceforge.net/">http://opennlp.sourceforge.net/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b5"></a><p class="hang">Bird, Steven, Edward Loper, and Rob Speer (Eds.) <i><span class="title">NLTK: Natural Language ToolKit</span></i>. &lt;<a href="http://nltk.sourceforge.net/">http://nltk.sourceforge.net/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b6"></a><p class="hang">Balpe, J.-P., and B. Magné (Eds.) (1991). <i><span class="title">L'Imagin-ation informatique de la littérature</span></i>. Saint-Denis: Presses Universitaires de Vincennes.
                  </p>
                  <a name="ss1-6-8_b7"></a><p class="hang">Braffort, P. (2006). "<i><span class="title">L'ALAMO: en avant 'post-'.</span></i>" &lt;<a href="http://paulbraffort.free.fr/litterature/alamo/alamo_avant_post.html">http://paulbraffort.free.fr/litterature/alamo/alamo_avant_post.html</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b8"></a><p class="hang">Carson, A. (1986). <i><span class="title">Eros the Bittersweet</span></i>. Princeton: Princeton UP.
                  </p>
                  <a name="ss1-6-8_b9"></a><p class="hang">Crane, G. (Ed.) (2006). <i><span class="title">The Perseus Digital Library</span></i>. &lt;<a href="http://www.perseus.org">http://www.perseus.org</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b10"></a><p class="hang">D'Antin Van Rooten, L. (1968). <i><span class="title">Mots D'Heures: Gousses, Rames The D'Antin Manuscript</span></i>. London: Angus &amp; Robertson.
                  </p>
                  <a name="ss1-6-8_b11"></a><p class="hang">Davidson, J. (2006). <i><span class="title">Truespel Converter</span></i>. &lt;<a href="http://www.truespel.com/en/">http://www.truespel.com/en/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b12"></a><p class="hang">Derrida, J. (1992). "<i><span class="title">La Pharmacie de Platon</span></i>" <i><span class="title">Plato's Pharmacy]</span></i>. In L. Brisson (Ed.). <i><span class="title">Phèdre suivi de La Pharmacie de Platon</span></i>. Paris: Gallimard.
                  </p>
                  <a name="ss1-6-8_b13"></a><p class="hang">Dreifus, C. (1998). "<i><span class="title">Got Stuck for a Moment: An Interview with Marvin Minsky.</span></i>" <i><span class="title">The International Herald Tribune</span></i> August.
                  </p>
                  <a name="ss1-6-8_b14"></a><p class="hang">Dunn, M. (2001). <i><span class="title">Ella Minnow Pea</span></i>. New York: Anchor Books.
                  </p>
                  <a name="ss1-6-8_b15"></a><p class="hang">Fellbaum, C. (Ed.) (1998). <i><span class="title">WordNet: An Electronic Lexical Database</span></i>. Cambridge, MA: MIT Press.
                  </p>
                  <a name="ss1-6-8_b16"></a><p class="hang">Hoban, R. (1980). <i><span class="title">Riddley Walker</span></i>. New York: Summit Books/Simon &amp; Schuster.
                  </p>
                  <a name="ss1-6-8_b17"></a><p class="hang">King, Stephen (2000). <i><span class="title">On Writing: A Memoir of the Craft</span></i>. New York: Scribner.
                  </p>
                  <a name="ss1-6-8_b18"></a><p class="hang">Kurzweil, R. (2006). "<i><span class="title">Kurzweil CyberArt Technologies.</span></i>" &lt;<a href="http://www.kurzweilcyberart.com/">http://www.kurzweilcyberart.com/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b19"></a><p class="hang">Lessard, Greg, and Levison, Michael. <i><span class="title">VINCI Laboratory</span></i>. &lt;<a href="http://www.cs.queensu.ca/CompLing/">http://www.cs.queensu.ca/CompLing/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b20"></a><p class="hang">Liu, H., and P. Singh (2002). "<i><span class="title">MAKEBELIEVE: Using Commonsense to Generate Stories.</span></i>" In <i><span class="title">Proceedings of the Eighteenth National Conference on Artificial Intelligence, AAAI 2002, July 28 –August 1, 2002, Edmonton, Alberta, Canada, 2002</span></i>, pp. 957–8. 
                  </p>
                  <a name="ss1-6-8_b21"></a><p class="hang">Liu, H., and P. Singh (forthcoming). "<i><span class="title">ConceptNet: A Practical Commonsense Reasoning Tool-kit</span></i>." <i><span class="title">BT Technology Journal</span></i> 22, forthcoming, Kluwer Academic Publishers. &lt;<a href="http://web.media.mit.edu/-hugo/publications/papers/BTTJ-ConceptNet.pdf">http://web.media.mit.edu/-hugo/publications/papers/BTTJ-ConceptNet.pdf</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b22"></a><p class="hang">Liu, H., P. Singh, and I. Eslick (2006). <i><span class="title">ConceptNet v.2.1</span></i>. &lt;<a href="http://web.media.mit.edu/-hugo/conceptnet/">http://web.media.mit.edu/-hugo/conceptnet/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b23"></a><p class="hang">Malsky, W. (2006). <i><span class="title">Why's (Poignant) Guide to Ruby</span></i>. &lt;<a href="http://www.poignantguide.net/ruby/">http://www.poignantguide.net/ruby/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b24"></a><p class="hang">Miller, George (Ed.) (2006). <i><span class="title">WordNet: An Electronic Lexical Database</span></i>. &lt;<a href="http://wordnet.princeton.edu/">http://wordnet.princeton.edu/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b25"></a><p class="hang">Mueller, E. T. (1999). "<i><span class="title">A Database and Lexicon of Scripts for <i>Thought Treasure</i>.</span></i>" &lt;<a href="http://www.signiform.com/tt/htm/script.htm">http://www.signiform.com/tt/htm/script.htm</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b26"></a><p class="hang">Mueller, E. T. (2000). <i><span class="title">Thought Treasure</span></i>. &lt;<a href="http://www.signiform.com/tt/htm/tt.htm">http://www.signiform.com/tt/htm/tt.htm</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b27"></a><p class="hang">Mueller, E. T. (2006). "<i><span class="title">Story Understanding Resources.</span></i>" &lt;<a href="http://xenia.media.mit.edu/-mueller/storyund/storyres.html">http://xenia.media.mit.edu/-mueller/storyund/storyres.html</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b28"></a><p class="hang">Pedersen, T. (2006). <i><span class="title">SenseRelate</span></i>. &lt;<a href="http://www.d.umn.edu/-tpederse/senserelate.html&gt;and&lt;http://senserelate.sourceforge.net/">http://www.d.umn.edu/-tpederse/senserelate.html&gt;and&lt;http://senserelate.sourceforge.net/</a>&gt;. Accessed April 2007.
                  </p>
                  <a name="ss1-6-8_b29"></a><p class="hang">Ponge, F. (1999). <i><span class="title">Tome Premier</span></i>. Paris: Gallimard.
                  </p>
                  <a name="ss1-6-8_b30"></a><p class="hang">Reiter, E. (2002). <i><span class="title">KPML</span></i>. &lt;<a href="http://www.fb10.uni-bremen.de/anglistik/langpro/kpml/README.html">http://www.fb10.uni-bremen.de/anglistik/langpro/kpml/README.html</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b31"></a><p class="hang">Reyes, R. (2006). "<i><span class="title"><i>Charabia</i>: Essais philosophi-ques.</span></i>" &lt;<a href="http://www.charabia.net/gen/gendisp.php?gen=1&amp;big=1&amp;font=l">http://www.charabia.net/gen/gendisp.php?gen 1&amp;big 1&amp;font l</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b32"></a><p class="hang">Roubaud, J. (1985). "<i><span class="title">Prothèse.</span></i>" in J.-F. Lyotard and T. Chaput (Eds.). <i><span class="title">Épreuves d'écriture</span></i>. Paris: Éditions du Centre Georges Pompidou.
                  </p>
                  <a name="ss1-6-8_b33"></a><p class="hang">Roubaud, J., and P. Lusson (2006). <i><span class="title">Rimbaudelaire</span></i>. &lt;<a href="http://alamo.mshparisnord.net/rialt/rimbaud.html">http://alamo.mshparisnord.net/rialt/rimbaud.html</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b34"></a><p class="hang">Round, M. (2006). <i><span class="title">Malvole Text Generator</span></i>. &lt;<a href="http://www.malevole.com/mv/misc/text/">http://www.malevole.com/mv/misc/text/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b35"></a><p class="hang">Singh, P. (2006a). "<i><span class="title">Open Mind Common Sense Data.</span></i>" &lt;<a href="http://csc.media.mit.edu/omcsraw_id.txt.gz">http://csc.media.mit.edu/omcsraw_id.txt.gz</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b36"></a><p class="hang">Singh, P. (2006b). "<i><span class="title">Open Mind Experiences.</span></i>" &lt;<a href="http://csc.media.mit.edu/OMEXHome.htm">http://csc.media.mit.edu/OMEXHome.htm</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b37"></a><p class="hang">Singh, P., T. Lin, E. T. Mueller, G. Lim, T. Perkins, and W. L. Zhu (2002). "<i><span class="title">Open Mind Common Sense: Knowledge Acquisition from the General Public.</span></i>" In <i><span class="title">Proceedings of the First International Conference on Ontologies, Databases, and Applications of Semantics for Large Scale
                           Information Systems, Irvine, California</span></i>. &lt;<a href="http://web.media.mit.edu/push/ODBASE2002.pdf">http://web.media.mit.edu/push/ODBASE2002.pdf</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b38"></a><p class="hang">Stein, A. (2006). <i><span class="title">TreeTagger</span></i>. &lt;<a href="http://www.uni-stuttgart.de/lingrom/stein/forschung/resource.html">http://www.uni-stuttgart.de/lingrom/stein/forschung/resource.html</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b39"></a><p class="hang">UCREL: University Centre for Computer Corpus Research on Language at the University of Lancaster (2006). <i><span class="title">CLAWS</span></i>. &lt;<a href="http://www.comp.lancs.ac.uk/ucrel/claws/">http://www.comp.lancs.ac.uk/ucrel/claws/</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b40"></a><p class="hang">Westbury, C. (2006). <i><span class="title">JanusNode</span></i>. &lt;<a href="http://www.janusnode.com">http://www.janusnode.com</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b41"></a><p class="hang">Winder, W. (2002). "<i><span class="title">Industrial Text and French Neo-structuralism.</span></i>" <i><span class="title">Computers and the Humanities</span></i> 36.3: 295–306.
                  </p>
                  <a name="ss1-6-8_b42"></a><p class="hang">Winder, W. (2004). "<i><span class="title">Robotic Poetics.</span></i>" In S. Shreibman, R. Siemens, and J. Unsworth (Eds.). <i><span class="title">Blackwell Companion to Digital Humanities</span></i>. Oxford: Blackwell, pp. 448–68. 
                  </p>
                  <a name="ss1-6-8_b43"></a><p class="hang">Winder, W. (2006). <i><span class="title">Robotic Poetics</span></i>. &lt;<a href="http://edziza.arts.ubc.ca/winder/rp">http://edziza.arts.ubc.ca/winder/rp</a>&gt;. Accessed August 2006.
                  </p>
                  <a name="ss1-6-8_b44"></a><p class="hang">Winder, W. (forthcoming). "<i><span class="title">Linking Fancy unto Fancy: Towards a Semantic IDE for Cascading Summaries</span></i>." In G. Shawver and R. Siemens (Eds.). <i><span class="title">New Paths for Computing Humanists</span></i>. Toronto: University of Toronto Press.
                  </p>
                  
                  
                  
               </div>
            
